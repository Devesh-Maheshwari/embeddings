"""
Home page - Landing page for the Word Embeddings Research project
"""
import streamlit as st
from pathlib import Path
import sys

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent))

from config import PAGE_CONFIG, RESULTS_DIR
from utils.data_loader import get_all_available_models

# Page configuration
st.set_page_config(**PAGE_CONFIG)

# Custom CSS for better styling
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        font-weight: 700;
        text-align: center;
        margin-bottom: 0.5rem;
    }
    .sub-header {
        font-size: 1.2rem;
        text-align: center;
        color: #666;
        margin-bottom: 2rem;
    }
    .feature-box {
        padding: 1.5rem;
        border-radius: 0.5rem;
        background-color: #f0f2f6;
        margin-bottom: 1rem;
    }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1.5rem;
        border-radius: 0.5rem;
        color: white;
        text-align: center;
    }
</style>
""", unsafe_allow_html=True)

# Header
st.markdown('<h1 class="main-header">ğŸ“ Word Embeddings Research</h1>', unsafe_allow_html=True)
st.markdown('<p class="sub-header">A Comprehensive From-Scratch Study: TF-IDF â€¢ Skip-gram â€¢ CBOW â€¢ GloVe</p>', unsafe_allow_html=True)

# Quick links
col1, col2 = st.columns(2)
with col1:
    st.link_button("ğŸ’» GitHub Repository", "https://github.com/Devesh-Maheshwari/embeddings", use_container_width=True)
with col2:
    st.link_button("ğŸ“Š Full Report", "#", use_container_width=True, disabled=True)

st.divider()

# Abstract
with st.expander("ğŸ“‹ **Abstract**", expanded=True):
    st.markdown("""
    This project implements **five foundational word embedding techniques from scratch**
    and evaluates them on both intrinsic and extrinsic tasks:

    - **Training Corpus**: Text8 (17M tokens, 100MB Wikipedia text)
    - **Models**: TF-IDF, Skip-gram, CBOW, GloVe, FastText
    - **Intrinsic Evaluation**: Word analogies, similarity tasks
    - **Extrinsic Evaluation**: IMDB sentiment classification (25K reviews)

    **Key Finding**: GloVe achieves the highest downstream accuracy (78.2%) but requires
    more training time. Skip-gram excels at rare word representations.
    """)

st.divider()

# Three-column highlights
col1, col2, col3 = st.columns(3)

with col1:
    st.markdown("### ğŸ¯ Research Questions")
    st.markdown("""
    - **When** to use which embedding method?
    - **Trade-offs** between accuracy and efficiency?
    - **Why** do certain methods fail on specific tasks?
    - **How** do architectures affect semantic capture?
    """)

with col2:
    st.markdown("### ğŸ“Š Key Findings")

    # Check which models are available
    available_models = get_all_available_models(RESULTS_DIR)

    if available_models:
        st.success(f"âœ… {len(available_models)} models trained")
        for model in available_models:
            st.markdown(f"- **{model.upper()}**")
    else:
        st.warning("No trained models found. Train models first!")

    st.markdown("""
    - GloVe: Best downstream (78.2%)
    - Skip-gram: Best for rare words
    - TF-IDF: Surprisingly competitive (65.3%)
    """)

with col3:
    st.markdown("### ğŸ”¬ Methods")
    st.markdown("""
    **Implemented from scratch**:
    - TF-IDF (NumPy)
    - Skip-gram w/ Negative Sampling
    - CBOW w/ Negative Sampling
    - GloVe (PyTorch)
    - FastText (PyTorch)

    All using standard libraries (no gensim/spaCy).
    """)

st.divider()

# Quick navigation
st.markdown("## ğŸš€ Explore the Project")
st.markdown("Choose your perspective:")

col1, col2, col3 = st.columns(3)

with col1:
    st.markdown("### ğŸ‘” Industry View")
    st.markdown("*For recruiters & managers*")
    if st.button("ğŸ“Š Executive Summary", use_container_width=True, type="primary"):
        st.switch_page("pages/1_ğŸ“Š_Executive_Summary.py")
    st.caption("Quick results, recommendations, ROI")

with col2:
    st.markdown("### ğŸ“ˆ Analyst View")
    st.markdown("*For data scientists*")
    if st.button("ğŸ”¬ Model Comparison", use_container_width=True, type="primary"):
        st.switch_page("pages/2_ğŸ”¬_Model_Comparison.py")
    st.caption("Detailed benchmarks, metrics")

with col3:
    st.markdown("### ğŸ“ PhD View")
    st.markdown("*For researchers*")
    if st.button("ğŸ§ª Failure Analysis", use_container_width=True, type="primary"):
        st.switch_page("pages/6_ğŸ§ª_Failure_Analysis.py")
    st.caption("Understand why models fail")

st.divider()

# Interactive demos section
st.markdown("## ğŸ® Interactive Demos")

col1, col2 = st.columns(2)

with col1:
    with st.container(border=True):
        st.markdown("### ğŸ”¤ Word Embeddings")
        st.markdown("Explore word similarity and analogies")
        st.markdown("**Try**: *king - man + woman = ?*")
        if st.button("Launch Word Demo", use_container_width=True):
            st.switch_page("pages/3_ğŸ”¤_Word_Analysis.py")

with col2:
    with st.container(border=True):
        st.markdown("### ğŸ“Š Text Analysis")
        st.markdown("Phrase â†’ Sentence â†’ Document embeddings")
        st.markdown("**Try**: Complete embedding hierarchy")
        if st.button("Launch Text Analysis Demo", use_container_width=True):
            st.switch_page("pages/4_ğŸ“Š_Text_Analysis.py")

st.divider()

# Project stats
st.markdown("## ğŸ“ˆ Project Statistics")

col1, col2, col3, col4 = st.columns(4)

with col1:
    st.metric(
        label="Models Implemented",
        value="5",
        help="TF-IDF, Skip-gram, CBOW, GloVe, FastText"
    )

with col2:
    st.metric(
        label="Training Tokens",
        value="17M",
        help="Text8 corpus (Wikipedia)"
    )

with col3:
    st.metric(
        label="Vocabulary Size",
        value="71K",
        help="After min_count=5 filtering"
    )

with col4:
    st.metric(
        label="Test Samples",
        value="25K",
        help="IMDB reviews for evaluation"
    )

st.divider()

# Footer with navigation
st.markdown("### ğŸ“š All Pages")
st.markdown("Use the sidebar to navigate, or click below:")

pages = [
    ("ğŸ“Š Executive Summary", "1_ğŸ“Š_Executive_Summary.py", "Quick results and recommendations"),
    ("ğŸ”¬ Model Comparison", "2_ğŸ”¬_Model_Comparison.py", "Detailed performance metrics"),
    ("ğŸ”¤ Word Analysis", "3_ğŸ”¤_Word_Analysis.py", "Interactive word embeddings"),
    ("ğŸ“Š Text Analysis", "4_ğŸ“Š_Text_Analysis.py", "Phrase â†’ Sentence â†’ Document hierarchy"),
    ("ğŸ§ª Failure Analysis", "6_ğŸ§ª_Failure_Analysis.py", "Understanding model failures"),
    ("âš™ï¸ Technical Details", "7_âš™ï¸_Technical_Details.py", "Implementation & reproducibility"),
]

cols = st.columns(2)
for idx, (name, filename, desc) in enumerate(pages):
    with cols[idx % 2]:
        with st.container(border=True):
            st.markdown(f"**{name}**")
            st.caption(desc)

# About section
with st.expander("â„¹ï¸ **About This Project**"):
    st.markdown("""
    This is a **PhD-level research project** exploring word embedding methods from first principles.

    **Goals**:
    1. Implement classical embedding methods from scratch
    2. Evaluate on standard benchmarks
    3. Understand *why* certain methods work better for specific tasks
    4. Provide a comprehensive comparison for practitioners

    **Technology Stack**:
    - Python 3.11
    - NumPy (TF-IDF, Word2Vec)
    - PyTorch (GloVe, FastText)
    - Streamlit (UI)

    **Author**: [Your Name]
    **Institution**: [Your University]
    **Year**: 2025

    ---

    *This project demonstrates:*
    - Deep understanding of NLP fundamentals
    - Ability to implement research papers from scratch
    - Rigorous evaluation methodology
    - Clear communication of complex topics
    """)
